TB3MS <- ts(USMacroSWQ$TB3MS, start = c(1957, 1),
end = c(2013, 4), frequency = 4)
# 10-years Treasury bonds interest rate as a 'ts' object
TB10YS <- ts(USMacroSWQ$GS10, start = c(1957, 1),
end = c(2013, 4), frequency = 4)
# generate the term spread series
TSpread <- TB10YS - TB3MS
# Estimate both equations using 'dynlm()'
VAR_EQ1 <- dynlm(GDPGrowth ~ L(GDPGrowth, 1:2) + L(TSpread, 1:2), start = c(1981, 1),
end = c(2012, 4))
VAR_EQ2 <- dynlm(TSpread ~ L(GDPGrowth, 1:2) + L(TSpread, 1:2), start = c(1981, 1),
end = c(2012, 4))
# rename regressors for better readability
names(VAR_EQ1$coefficients) <- c("Intercept","Growth_t-1",
"Growth_t-2", "TSpread_t-1", "TSpread_t-2")
names(VAR_EQ2$coefficients) <- names(VAR_EQ1$coefficients)
# robust coefficient summaries
coeftest(VAR_EQ1, vcov. = sandwich)
coeftest(VAR_EQ2, vcov. = sandwich)
# set up data for estimation using `VAR()`
VAR_data <- window(ts.union(GDPGrowth, TSpread), start = c(1980, 3), end = c(2012, 4))
# estimate model coefficients using `VAR()`
VAR_est <- VAR(y = VAR_data, p = 2) VAR_est
# estimate model coefficients using `VAR()`
VAR_est <- VAR(y = VAR_data, p = 2)
VAR_est
# obtain the adj. R^2 from the output of 'VAR()'
summary(VAR_est$varresult$GDPGrowth)$adj.r.squared
summary(VAR_est$varresult$TSpread)$adj.r.squared
# estimate model coefficients using `VAR()`
VAR_est <- VAR(y = VAR_data, p = 2)
install.packages("vars")
# estimate model coefficients using `VAR()`
VAR_est <- VAR(y = VAR_data, p = 2)
library(vars)
# estimate model coefficients using `VAR()`
VAR_est <- VAR(y = VAR_data, p = 2)
VAR_est
# obtain the adj. R^2 from the output of 'VAR()'
summary(VAR_est$varresult$GDPGrowth)$adj.r.squared
summary(VAR_est$varresult$TSpread)$adj.r.squared
# Granger causality tests:
# test if term spread has no power in explaining GDP growth
linearHypothesis(VAR_EQ1,
hypothesis.matrix = c("TSpread_t-1", "TSpread_t-2"),
vcov. = sandwich)
# test if GDP growth has no power in explaining term spread
linearHypothesis(VAR_EQ2,
hypothesis.matrix = c("Growth_t-1", "Growth_t-2"),
vcov. = sandwich)
# compute iterated forecasts for GDP growth and term spread for the next 10 quarters
forecasts <- predict(VAR_est) forecasts
# compute iterated forecasts for GDP growth and term spread for the next 10 quarters
forecasts <- predict(VAR_est)
forecasts
# visualize the iterated forecasts
plot(forecasts)
# estimate models for direct two-quarter-ahead forecasts
VAR_EQ1_direct <- dynlm(GDPGrowth ~ L(GDPGrowth, 2:3)
+ L(TSpread, 2:3), start = c(1981, 1), end = c(2012, 4))
VAR_EQ2_direct <- dynlm(TSpread ~ L(GDPGrowth, 2:3)
+ L(TSpread, 2:3), start = c(1981, 1), end = c(2012, 4))
# compute direct two-quarter-ahead forecasts
coef(VAR_EQ1_direct) %*% c(1,       # intercept
window(GDPGrowth, start = c(2012, 3), end = c(2012, 4)),
window(TSpread, start = c(2012, 3), end = c(2012, 4)))
coef(VAR_EQ2_direct) %*% c(1, # intercept
window(GDPGrowth, start = c(2012, 3), end = c(2012, 4)),
window(TSpread, start = c(2012, 3), end = c(2012, 4)))
#### Orders of Integration and the DF-GLS Unit Root Test ####
# define ts object of the U.S. PCE Price Index
PCECTPI <- ts(log(USMacroSWQ$PCECTPI), start = c(1957, 1),
end = c(2012, 4), freq = 4)
# plot logarithm of the PCE Price Index
plot(log(PCECTPI),
main = "Log of United States PCE Price Index", ylab = "Logarithm",
col = "steelblue",
lwd = 2)
# plot U.S. PCE price inflation
plot(400 * Delt(PCECTPI),
main = "United States PCE Price Index", ylab = "Percent per annum",
col = "steelblue",
lwd = 2)
# add a dashed line at y= 0
abline(0, 0, lty = 2)
# DF-GLS test for unit root in GDP
summary(ur.ers(log(window(GDP, start = c(1962, 1), end = c(2012, 4))),  model = "trend",
lag.max = 2))
#### Cointegration ####
# reproduce Figure 16.2 of the book
# plot both interest series
plot(merge(as.zoo(TB3MS), as.zoo(TB10YS)), plot.type = "single",
lty = c(2, 1),
lwd = 2,
xlab = "Date",
ylab = "Percent per annum", ylim = c(-5, 17),
main = "Interest Rates")
# add the term spread series
lines(as.zoo(TSpread), col = "steelblue",
lwd = 2,
xlab = "Date",
ylab = "Percent per annum",
main = "Term Spread")
# shade the term spread
polygon(c(time(TB3MS), rev(time(TB3MS))), c(TB10YS, rev(TB3MS)),
col = alpha("steelblue", alpha = 0.3), border = NA)
# add horizontal line add 0
abline(0, 0)
# add a legend
legend("topright",
legend = c("TB3MS", "TB10YS", "Term Spread"), col = c("black", "black", "steelblue"),
lwd = c(2, 2, 2),
lty = c(2, 1, 1))
# test for nonstationarity of 3-month treasury bills using ADF test
ur.df(window(TB3MS, c(1962, 1), c(2012, 4)), lags = 6,
selectlags = "AIC",
type = "drift")
#  test for nonstationarity of 10-years treasury bonds using ADF test
ur.df(window(TB10YS, c(1962, 1), c(2012, 4)), lags = 6,
selectlags = "AIC",
type = "drift")
# test for nonstationarity of 3-month treasury bills using DF-GLS test
ur.ers(window(TB3MS, c(1962, 1), c(2012, 4)), model = "constant",
lag.max = 6)
# test for nonstationarity of 10-years treasury bonds using DF-GLS test
ur.ers(window(TB10YS, c(1962, 1), c(2012, 4)), model = "constant",
lag.max = 6)
# test if term spread is stationairy (cointegration of interest rates) using ADF
ur.df(window(TB10YS, c(1962, 1), c(2012, 4)) - window(TB3MS, c(1962, 1), c(2012 ,4)), lags = 6,
selectlags = "AIC",
type = "drift")
# test if term spread is stationairy (cointegration of interest rates) using DF-GLS test
ur.ers(window(TB10YS, c(1962, 1), c(2012, 4)) - window(TB3MS, c(1962, 1), c(2012, 4)),
model = "constant",
lag.max = 6)
# estimate first-stage regression of EG-ADF test
FS_EGADF <- dynlm(window(TB10YS, c(1962, 1), c(2012, 4)) ~ window(TB3MS, c(1962, 1), c(2012,4)))
FS_EGADF
# compute the residuals
z_hat <- resid(FS_EGADF)
# compute the ADF test statistic
ur.df(z_hat, lags = 6, type = "none", selectlags = "AIC")
TB10YS <- window(TB10YS, c(1962, 1), c(2012 ,4)) TB3MS <- window(TB3MS, c(1962, 1), c(2012, 4))
TB10YS <- window(TB10YS, c(1962, 1), c(2012 ,4))
TB3MS <- window(TB3MS, c(1962, 1), c(2012, 4))
# set up error correction term
VECM_ECT <- TB10YS - TB3MS
# estimate both equations of the VECM using 'dynlm()'
VECM_EQ1 <- dynlm(d(TB10YS) ~ L(d(TB3MS), 1:2) + L(d(TB10YS), 1:2) + L(VECM_ECT))
VECM_EQ2 <- dynlm(d(TB3MS) ~ L(d(TB3MS), 1:2) + L(d(TB10YS), 1:2) + L(VECM_ECT))
# rename regressors for better readability
names(VECM_EQ1$coefficients) <- c("Intercept", "D_TB3MS_l1",
"D_TB3MS_l2", "D_TB10YS_l1", "D_TB10YS_l2", "ect_l1")
names(VECM_EQ2$coefficients) <- names(VECM_EQ1$coefficients)
# coefficient summaries using HAC standard errors
coeftest(VECM_EQ1, vcov. = NeweyWest(VECM_EQ1, prewhite = F, adjust = T))
coeftest(VECM_EQ2, vcov. = NeweyWest(VECM_EQ2, prewhite = F, adjust = T))
#### Volatility Clustering and Autoregressive Conditional Het- eroskedasticity ####
# import data on the Wilshire 5000 index
file.choose()
W5000 <- read.csv2("/Users/nicoleyin88/Documents/Other /1. R/1. R Tutorial/Willshire500.csv",
stringsAsFactors = F,
header = T,
sep = ",",
na.strings = ".")
# transform the columns
W5000$DATE <- as.Date(W5000$DATE)
W5000$WILL5000INDFC <- as.numeric(W5000$WILL5000INDFC)
# remove NAs
W5000 <- na.omit(W5000)
# compute daily percentage changes
W5000_PC <- data.frame("Date" = W5000$DATE,
"Value" = as.numeric(Delt(W5000$WILL5000INDFC) * 100))
W5000_PC <- na.omit(W5000_PC)
# plot percentage changes
plot(W5000_PC,
ylab = "Percent",
main = "Daily Percentage Changes",
type="l",
col = "steelblue",
lwd = 0.5)
# add horizontal line at y = 0
abline(0, 0)
# plot sample autocorrelation of daily percentage changes
acf(W5000_PC$Value, main = "Wilshire 5000 Series")
# estimate GARCH(1,1) model of daily percentage changes
GARCH_Wilshire <- garchFit(data = W5000_PC$Value, trace = F)
# plot sample autocorrelation of daily percentage changes
acf(W5000_PC$Value, main = "Wilshire 5000 Series")
# estimate GARCH(1,1) model of daily percentage changes
GARCH_Wilshire <- garchFit(data = W5000_PC$Value, trace = F)
library(vars)
install.packages("fGarch")
library(fGarch)
# estimate GARCH(1,1) model of daily percentage changes
GARCH_Wilshire <- garchFit(data = W5000_PC$Value, trace = F)
# compute deviations of the percentage changes from their mean
dev_mean_W5000_PC <- W5000_PC$Value - GARCH_Wilshire@fit$coef[1]
# plot deviation of percentage changes from mean
plot(W5000_PC$Date, dev_mean_W5000_PC, type = "l",
col = "steelblue",
ylab = "Percent",
xlab = "Date",
main = "Estimated Bands of +- One Conditional Standard Deviation",
lwd = 0.2)
# add horizontal line at y = 0
abline(0, 0)
# add GARCH(1,1) confidence bands (one standard deviation) to the plot
lines(W5000_PC$Date,
GARCH_Wilshire@fit$coef[1] + GARCH_Wilshire@sigma.t, col = "darkred",
lwd = 0.5)
lines(W5000_PC$Date,
GARCH_Wilshire@fit$coef[1] - GARCH_Wilshire@sigma.t, col = "darkred",
lwd = 0.5)
library(flexdashboard); library(NHANES); library(plotly);library(dplyr)
---
title: "How does your BMI measure up?"
output: flexdashboard::flex_dashboard
runtime: shiny
---
Inputs {.sidebar}
-------------------------------------
```{r}
library(flexdashboard); library(NHANES); library(plotly);library(dplyr)
sliderInput("height", "Height in inches",0,100,72)
sliderInput("weight", "Weight in pounds",0,500,100)
sliderInput("age", "Age in years",0,120,50)
```
Column
-------------------------------------
### Chart 1
```{r}
nhanes = sample_n(NHANES,100)
renderPlotly({
df = data.frame(bmi = c(nhanes$BMI,input$weight*0.45/(input$height*0.025)^2),
age = c(nhanes$Age,input$age),
who = c(rep("nhanes",100),"you"))
ggplotly(ggplot(df) +
geom_point(aes(x=age,y=bmi,color=who)) +
scale_x_continuous(limits=c(0,90)) +
scale_y_continuous(limits=c(0,60)) +
theme_minimal()
)
})
```
install.packages("flexdashboard")
install.packages("NHANES")
install.packages("plotly")
---
title: "How does your BMI measure up?"
output: flexdashboard::flex_dashboard
runtime: shiny
---
Inputs {.sidebar}
-------------------------------------
```{r}
library(flexdashboard); library(NHANES); library(plotly);library(dplyr)
sliderInput("height", "Height in inches",0,100,72)
sliderInput("weight", "Weight in pounds",0,500,100)
sliderInput("age", "Age in years",0,120,50)
```
Column
-------------------------------------
### Chart 1
```{r}
nhanes = sample_n(NHANES,100)
renderPlotly({
df = data.frame(bmi = c(nhanes$BMI,input$weight*0.45/(input$height*0.025)^2),
age = c(nhanes$Age,input$age),
who = c(rep("nhanes",100),"you"))
ggplotly(ggplot(df) +
geom_point(aes(x=age,y=bmi,color=who)) +
scale_x_continuous(limits=c(0,90)) +
scale_y_continuous(limits=c(0,60)) +
theme_minimal()
)
})
```
library(flexdashboard); library(NHANES); library(plotly);library(dplyr)
sliderInput("height", "Height in inches",0,100,72)
library(flexdashboard); library(NHANES); library(plotly);library(dplyr)
sliderInput("height", "Height in inches",0,100,72)
install.packages("shinybootstrap2")
library(shinybootstrap2)
library(flexdashboard); library(NHANES); library(plotly);library(dplyr)
sliderInput("height", "Height in inches",0,100,72)
sliderInput("weight", "Weight in pounds",0,500,100)
sliderInput("age", "Age in years",0,120,50)
---
title: "How does your BMI measure up?"
output: flexdashboard::flex_dashboard
runtime: shiny
---
Inputs {.sidebar}
-------------------------------------
```{r}
library(flexdashboard); library(NHANES); library(plotly);library(dplyr)
sliderInput("height", "Height in inches",0,100,72)
sliderInput("weight", "Weight in pounds",0,500,100)
sliderInput("age", "Age in years",0,120,50)
```
Column
-------------------------------------
### Chart 1
```{r}
nhanes = sample_n(NHANES,100)
renderPlotly({
df = data.frame(bmi = c(nhanes$BMI,input$weight*0.45/(input$height*0.025)^2),
age = c(nhanes$Age,input$age),
who = c(rep("nhanes",100),"you"))
ggplotly(ggplot(df) +
geom_point(aes(x=age,y=bmi,color=who)) +
scale_x_continuous(limits=c(0,90)) +
scale_y_continuous(limits=c(0,60)) +
theme_minimal()
)
})
```
sliderInput("age", "Age in years",0,120,50)
Column
-------------------------------------
nhanes = sample_n(NHANES,100)
renderPlotly({
df = data.frame(bmi = c(nhanes$BMI,input$weight*0.45/(input$height*0.025)^2),
age = c(nhanes$Age,input$age),
who = c(rep("nhanes",100),"you"))
ggplotly(ggplot(df) +
geom_point(aes(x=age,y=bmi,color=who)) +
scale_x_continuous(limits=c(0,90)) +
scale_y_continuous(limits=c(0,60)) +
theme_minimal()
)
})
nhanes = sample_n(NHANES,100)
renderPlotly({
df = data.frame(bmi = c(nhanes$BMI,input$weight*0.45/(input$height*0.025)^2),
age = c(nhanes$Age,input$age),
who = c(rep("nhanes",100),"you"))
ggplotly(ggplot(df) +
geom_point(aes(x=age,y=bmi,color=who)) +
scale_x_continuous(limits=c(0,90)) +
scale_y_continuous(limits=c(0,60)) +
theme_minimal()
)
})
install.packages("Momocs")
library(Momocs)
nhanes = sample_n(NHANES,100)
renderPlotly({
df = data.frame(bmi = c(nhanes$BMI,input$weight*0.45/(input$height*0.025)^2),
age = c(nhanes$Age,input$age),
who = c(rep("nhanes",100),"you"))
ggplotly(ggplot(df) +
geom_point(aes(x=age,y=bmi,color=who)) +
scale_x_continuous(limits=c(0,90)) +
scale_y_continuous(limits=c(0,60)) +
theme_minimal()
)
})
nhanes = sample_n(NHANES,100)
renderPlotly({
df = data.frame(bmi = c(nhanes$BMI,input$weight*0.45/(input$height*0.025)^2),
age = c(nhanes$Age,input$age),
who = c(rep("nhanes",100),"you"))
ggplotly(ggplot(df) +
geom_point(aes(x=age,y=bmi,color=who)) +
scale_x_continuous(limits=c(0,90)) +
scale_y_continuous(limits=c(0,60)) +
theme_minimal()
)
})
library(stats)
library(stats)
?stats
library(plm)
?plm
modelLookup("plm")
library(xgboost)
install.packages("xgboost")
detach("package:plm", unload = TRUE)
library(xgboost)
modelLookup("xgbTree")
install.packages("caret")
library(caret)
modelLookup("xgbTree")
library(caret)
install.packages("caret")
library(caret)
library(caret)
library(caret)
modelLookup("xgbTree")
library(tidyverse)
file.choose()
income <- read_csv("income.csv", col_types = "nffnfffffnff")
setwd("/Users/nicoleyin88/Documents/Other /* Programming/2. Machine Learning/Data/")
income <- read_csv("income.csv", col_types = "nffnfffffnff")
library(caret)
# 2. create training and test data
set.seed(1234)
sample_set <- createDataPartition(y=income$income, p=0.75, list=FALSE)
income_train <- income[sample_set,]
income_test <- income[-sample_set,]
library(DMwR)
set.seed(1234)
set.seed(1234)
income_train <- SMOTE(income ~ .,
data.frame(income_train),
perc.over = 100,
perc.under = 200)
# 4. generate k=5
libraray(rpart)
install.packages("rpart")
# 4. generate k=5
libraray(rpart)
install.packages("rpart")
# 4. generate k=5
libraray(rpart)
library(rpart)
set.seed(1234)
income_mod <- train(income ~ .,
data=income_train,
metric="Accuracy",
method="rpart",
trControl=trainControl(method="cv", number=5))
library(caret)
library(tidyverse)
library(DMwR)
income_mod <- train(income ~ .,
data=income_train,
metric="Accuracy",
method="rpart",
trControl=trainControl(method="cv", number=5))
# 5. see performance
income_mod$resample %>%
arrange(Resample)
# 6. see accuracy
income_mod$resample %>%
arrange(Resample) %>%
summarise(AvgAccuracy=mean(Accuracy))
ls(income)
# 4. k=5 fold cross-validation
# (1) degree = 1
library(rpart)
set.seed(1234)
income_mod <- train(income ~ age + educationLevel + educationYears,
data=income_train,
metric="Accuracy",
method="rpart",
trControl=trainControl(method="cv", number=5))
# 5. see performance
income_mod$resample %>%
arrange(Resample)
summary(incoome_mod)
# 4. k=5 fold cross-validation
# (1) degree = 1
library(rpart)
set.seed(1234)
income_mod1 <- train(income ~ age + educationLevel + educationYears,
data=income_train,
metric="Accuracy",
method="rpart",
trControl=trainControl(method="cv", number=5))
summary(income_mod1)
ls(income_mod1)
# 5. see performance
income_mod$resample %>%
arrange(Resample)
# 5. see performance
income_mod1$resample %>%
arrange(Resample)
# 6. see accuracy
income_mod1$resample %>%
arrange(Resample) %>%
summarise(AvgAccuracy=mean(Accuracy))
# (2) degree = 2
income_mod2 <- train(income ~ I(age^2) + I(educationLevel^2) + I(educationYears^2),
data=income_train,
metric="Accuracy",
method="rpart",
trControl=trainControl(method="cv", number=5))
# (2) degree = 2
income_mod2 <- train(income ~ I(age^2) + educationLevel + I(educationYears^2),
data=income_train,
metric="Accuracy",
method="rpart",
trControl=trainControl(method="cv", number=5))
summary(income_mod2)
# see performance
income_mod2$resample %>%
arrange(Resample)
# see accuracy
income_mod2$resample %>%
arrange(Resample) %>%
summarise(AvgAccuracy=mean(Accuracy))
# see accuracy
income_mod1$resample %>%
arrange(Resample) %>%
summarise(AvgAccuracy=mean(Accuracy))
mse1 <- sum((income$income - predict(income_mod1))^2)
predict1 <- predict(income_mod1)
head(income$income)
head(predict1)
factor(predict1)
dummy_cols(income$income)
install.packages("fastDummies")
library(fastDummies)
dummy_cols(income$income)
income1 <- dummy_cols(income$income)
predict1 <- dummy_cols(predict(income_mod1))
mse1 <- sum((income1 - predict1)^2)
dim(income1)
dim(predict1)
